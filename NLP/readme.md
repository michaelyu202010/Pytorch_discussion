说明: 

从处理方法来看, 图像和NLP是不同的. 图像用卷积神经网络(CNN), 已经比较成功了;

NLP以前一直没有好办法, 用过循环神经网络(RNN), 长短期记忆(LSTM), 最后出现了Transformer的办法, 才算成功了. 

比较好的顺序是:    
1. 先理解NLP的基本方法, RNN/LSTM简单了解即可.  
2. 然后理解Transformer 
3. 重点理解生成式模型(Generative mode)

*生成式模型是神经网络通过学习, 自身产生数据, 这会让AI快速发展起来. 想象一下:在任何领域, 先提供一些代表性的数据, 生成式模型A就可以开始学习, 并产生数据. 这些数据用来训练另外一个模型B, 并把训练结果反馈给原来的模型A, 模型A调整后再生成数据给B...  一开始产生的数据可能不完善, 但是只要不断循环下去, 最后的结果会逼近一个均衡点 - 就是两个模型都接近完美. 这是现在大火的GAN(生成式对抗网络). 

任何领域(画图, 写作, 经济, 科技, 游戏)都可以用这个办法, 想象空间非常大. 

